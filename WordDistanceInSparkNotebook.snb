{"metadata":{"name":"WordDistanceHomeworkJellePelfrene","user_save_timestamp":"1970-01-01T01:00:00.000Z","auto_save_timestamp":"1970-01-01T01:00:00.000Z","language_info":{"name":"scala","file_extension":"scala","codemirror_mode":"text/x-scala"},"trusted":true,"customLocalRepo":null,"customRepos":null,"customDeps":null,"customImports":null,"customSparkConf":{"spark.app.name":"Notebook","spark.master":"local[8]","spark.executor.memory":"1G"}},"cells":[{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"import org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\nimport org.apache.spark.rdd.RDD \n\n  type Hit = (Long, String) // (index, the significant word found at index)\n  type SameWordInterval = ((Long,Long),String) // ( (startIdx, endIdx), significant word)\n  \n  /**\n   * Finds the distance between given words in the text, defined as \n   * minimum amount of other words in between occurrences of the given words.\n   * \n   * Distance between a word and itself is 0.\n   * If one of the words does not occur in the text, distance is None\n   *  \n   * @param textFilePath\n   * @param word1\n   * @param word2\n   * @param sc sparkContext\n   * @return\n   */\n  def wordDistance(textFilePath:String, word1:String, word2:String)(implicit sc:SparkContext):Option[Long] = {\n    if (word1 == null || word2 == null) return None\n    \n    // need access to a cluster to tweak this experimentally, so this is just a guess. \n    // I don't know whether even to fiddle with this at all or just take the defaultParallelism.\n    implicit val nbPartitions = sc.defaultParallelism * 2\n        \n    // look at text file as sequence of words\n    val words:RDD[String] = asFlatWordList(textFilePath)\n    \n    // we filter out all words that are not word1 or word2\n    val sortedHits:RDD[Hit] = findAndSortHitsOf(words, word1, word2)\n    println(\"\\n ***** Nb hits: \"+sortedHits.count()+\" .\\n\")\n\n    // work straight on the hits, to accomodate also \"Let's call this a 'hard' case worth extra credits. Could: d(\"a b b b b b b b a\", a , a) == 7 ?\"\n    findSmallestDistanceBetweenWordsUsingHits(sortedHits, word1, word2)\n  }\n  \n  def asFlatWordList(textFilePath:String)(implicit sc:SparkContext, nbPartitions:Int):RDD[String] = {\n    sc.textFile(textFilePath, nbPartitions).flatMap { l => l.split(\" \") }\n  }\n\n  def findAndSortHitsOf(words:RDD[String], word1:String, word2:String)(implicit nbPartitions:Int):RDD[Hit] = {\n    // this part should be embarrassingly parallel, as we like it\n    val hits = words.zipWithIndex().filter(p=> (p._1 == word1 || p._1 == word2)).map(p => p.swap)\n    // the sort destroys the parallelism, but is necessary for avoiding unnecessary comparisons\n    // I'm assuming here that spark implements this sort well, in order to make this still an overall win.\n    val sorted = hits.sortByKey(ascending=true, numPartitions = nbPartitions)\n    sorted\n  }\n  \n  def findSmallestDistanceBetweenWordsUsingHits(orderedHits:RDD[Hit], word1:String, word2:String): Option[Long] = {\n    import org.apache.spark.mllib.rdd.RDDFunctions.fromRDD\n    val pimped = fromRDD(orderedHits)\n    val result = pimped.sliding(2).aggregate[Option[Long]](None)(\n      seqOp = (accum:Option[Long], window:Array[Hit]) => {\n        if (!( (window(0)._2 == word1 && window(1)._2 == word2 ) ||\n               (window(0)._2 == word2 && window(1)._2 == word1 )   )) {\n          accum\n        } else { \n          val dist = calcDist(window(0)._1, window(1)._1)\n          accum match {\n            case None => Some(dist)\n            case Some(oldDist) => Some(Math.min(oldDist, dist))\n          }\n        }\n      },\n      combOp = (accum1:Option[Long], accum2:Option[Long]) => (accum1,accum2) match {\n        case (Some(value1),Some(value2)) => Some(Math.min(value1,value2))\n        case (None, _)  => accum2\n        case (_, None) => accum1\n      } \n      ) \n    result\n  }\n\n  def calcDist(first:SameWordInterval, second:SameWordInterval):Long = calcDist(first._1._2, second._1._1)\n  def calcDist(first:Long, second:Long):Long = {\n    assert(second > first)\n    second-first-1\n  }\n  ","outputs":[{"name":"stdout","output_type":"stream","text":"import org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\nimport org.apache.spark.rdd.RDD\ndefined type alias Hit\ndefined type alias SameWordInterval\nwordDistance: (textFilePath: String, word1: String, word2: String)(implicit sc: org.apache.spark.SparkContext)Option[Long]\nasFlatWordList: (textFilePath: String)(implicit sc: org.apache.spark.SparkContext, implicit nbPartitions: Int)org.apache.spark.rdd.RDD[String]\nfindAndSortHitsOf: (words: org.apache.spark.rdd.RDD[String], word1: String, word2: String)(implicit nbPartitions: Int)org.apache.spark.rdd.RDD[(Long, String)]\nfindSmallestDistanceBetweenWordsUsingHits: (orderedHits: org.apache.spark.rdd.RDD[(Long, String)], word1: String, word2: String)Option[Long]\ncalcDist: (first: ((Long, Long..."},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":1}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"\n\n    implicit val sc:SparkContext = sparkContext\n    sc.version","outputs":[{"name":"stdout","output_type":"stream","text":"sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@7132699a\nres2: String = 1.4.0\n"},{"metadata":{},"data":{"text/html":"1.4.0"},"output_type":"execute_result","execution_count":3}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"    // practical test 3 is the \"extra-credit case\" can distance(\"a b b b b b b b a\",\"a\",\"a\") be 7\n    val testFileAbba = \"abbbbbba.txt\"\n    val minAtoA = wordDistance(testFileAbba, \"a\", \"a\")\n    println(\"\\n***** distance between 'a' and 'a' : \"+minAtoA+\"\\n\")\n    if (Some(7L) != minAtoA) {\n      throw new Exception(\"\"\"Distance between \"a\" and \"a\" found to be \"\"\"+minAtoA+\n          \"\"\". However, this was working on Jelle's machine on June 18th.\"\"\")\n    }","outputs":[{"name":"stdout","output_type":"stream","text":"\n ***** Nb hits: 2 .\n\n\n***** distance between 'a' and 'a' : Some(7)\n\ntestFileAbba: String = abbbbbba.txt\nminAtoA: Option[Long] = Some(7)\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":4}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"    // practical test 2 runs on the well-tokenized shakespeare corpus, \n    // which is downloadable at http://norvig.com/ngrams/shakespeare.txt\n    val testFileShakespeare = \"shakespeare.txt\"\n    val minRomeoToJuliet = wordDistance(testFileShakespeare, \"Romeo\", \"Juliet\")\n    println(\"\\n***** distance between Romeo and Juliet : \"+minRomeoToJuliet+\"\\n\")\n    // excerpt from shakespeare.txt: Is father , mother , Tybalt , Romeo , Juliet ,\n    if (Some(1L) != minRomeoToJuliet) {\n      throw new Exception(\"\"\"Distance between \"Romeo\" and \"Juliet found to be \"\"\"+minRomeoToJuliet+\n          \"\"\". However, this was working on Jelle's machine on June 12th.\"\"\")\n    }","outputs":[{"name":"stdout","output_type":"stream","text":"\n ***** Nb hits: 158 .\n\n\n***** distance between Romeo and Juliet : Some(1)\n\ntestFileShakespeare: String = shakespeare.txt\nminRomeoToJuliet: Option[Long] = Some(1)\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":5}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"\n    // practical test 1 runs on the readme file of the spark-notebook itself.\n    val testFileREADME = \"README-sparknotebook.md\" \n    val minSparkToVersion = wordDistance(testFileREADME, \"Spark\", \"version\")\n    println(\"\\n***** distance between Spark and version : \"+minSparkToVersion+\"\\n\")\n    if (Some(0L) != minSparkToVersion) {\n      throw new Exception(\"\"\"Distance between \"Spark\" and \"version\" found to be \"\"\"+minSparkToVersion+\n          \"\"\". However, this was working on Jelle's machine on June 12th on the README from spark-notebook/master.\"\"\")\n    }","outputs":[{"name":"stdout","output_type":"stream","text":"\n ***** Nb hits: 36 .\n\n\n***** distance between Spark and version : Some(0)\n\ntestFileREADME: String = README-sparknotebook.md\nminSparkToVersion: Option[Long] = Some(0)\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":6}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"","outputs":[]}],"nbformat":4}